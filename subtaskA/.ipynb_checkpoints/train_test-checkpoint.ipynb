{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"loads the data, pre-trainied embeddings, feature sets, and trains a voting classifier and subsequently tests the model\n",
    "    on the held-out test data\"\"\"\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "import gensim.models\n",
    "\n",
    "# import from the scripts provided by the creator(s) of Twitter Word2vec model to read pre-trained embeddings\n",
    "# source: https://www.fredericgodin.com/software/\n",
    "import word2vecReaderUtils as utils\n",
    "from word2vecReader import *\n",
    "\n",
    "from load import parse_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run the following, download the word2vec Twitter model and set its path in the body of the function  \n",
    "def bisectioned_embeddings_avg(corpus):\n",
    "    \"\"\"splits each tweet into 2 sections, averages word and emoji embeddings for each part separately\"\"\"\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True).tokenize\n",
    "    # SET THE PATH TO TWITTER WORD2VEC MODEL\n",
    "    wvModel = Word2Vec.load_word2vec_format('../install_dependencies/word2vec_twitter_model.bin', binary=True)\n",
    "    wvModel_size = wvModel.layer1_size\n",
    "    # SET THE PATH TO TWITTER EMOJI2VEC MODEL\n",
    "    emojiModel = gensim.models.KeyedVectors.load_word2vec_format('../install_dependencies/emoji2vec.bin', binary=True)\n",
    "    emojiModel_size = emojiModel.vector_size \n",
    "    meanVectors = []\n",
    "    for tweet in corpus:\n",
    "        t = tokenizer(tweet)\n",
    "        rightWords, rightEmojis, leftWords, leftEmojis = ([], [], [], [])\n",
    "         # meaning of the following variables:\n",
    "         # mRW: mean right words\n",
    "         # mRE: mean right emojis \n",
    "         # mLW: mean left words\n",
    "         # mLE: mean left emojis\n",
    "        mRW, mRE, mLW, mLE = (np.zeros(wvModel_size), np.zeros(emojiModel_size), np.zeros(wvModel_size), np.zeros(emojiModel_size)) \n",
    "        for i in range(int(len(t)/2)):\n",
    "            # don't look up embeddings for generic '@user'\n",
    "            if t[i] in wvModel and not t[i].startswith('@'):\n",
    "                rightWords.append(wvModel[t[i]])\n",
    "            if t[i] in emojiModel:\n",
    "                rightEmojis.append(emojiModel[t[i]])\n",
    "        for i in range(int(len(t)/2), len(t)):\n",
    "            if t[i] in wvModel and not t[i].startswith('@'):\n",
    "                leftWords.append(wvModel[t[i]])\n",
    "            if t[i] in emojiModel:\n",
    "                leftEmojis.append(emojiModel[t[i]])\n",
    "        if len(rightWords)>0:\n",
    "            mRW = np.mean(rightWords, axis=0)\n",
    "        if len(rightEmojis)>0:\n",
    "            mRE = np.mean(rightEmojis, axis=0)\n",
    "        if len(leftWords)>0:\n",
    "            mLW = np.mean(leftWords, axis=0)\n",
    "        if len(leftEmojis)>0:\n",
    "            mLE = np.mean(leftEmojis, axis=0)\n",
    "        # concatenate all the vector averages \n",
    "        meanVectors.append(np.concatenate((mRW, mRE, mLW, mLE)))\n",
    "            \n",
    "    return meanVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Experiment settings\n",
    "\n",
    "DATASET_FP = \"../datasets/train/SemEval2018-T3-train-taskA_emoji.txt\"\n",
    "TASK = \"A\" # Define, A or B\n",
    "FNAME = './predictions-task' + TASK + '.txt'\n",
    "PREDICTIONSFILE = open(FNAME, \"w\")\n",
    "EXTRA_FEATURES = 1 # we set this flag to 1 when we want to use handcrafted features in combination with embeddings \n",
    "\n",
    "K_FOLDS = 10 # 10-fold crossvalidation\n",
    "\n",
    "random_state=11\n",
    "# These CLF's are defined based on the code output of the original repository\n",
    "CLF1 = SVC(C=1.0, \n",
    "           cache_size=200, \n",
    "           class_weight=None, \n",
    "           coef0=0.0,\n",
    "           decision_function_shape=None, \n",
    "           degree=3, \n",
    "           gamma='auto', \n",
    "           kernel='rbf',\n",
    "           max_iter=-1, \n",
    "           probability=True, \n",
    "           random_state=random_state, \n",
    "           shrinking=True,\n",
    "           tol=0.001, \n",
    "           verbose=False)\n",
    "# Previous, less verbose code: SVC(random_state=random_state, probability=True)\n",
    "\n",
    "CLF2 = LogisticRegression(C=1.0, \n",
    "                          class_weight=None, \n",
    "                          dual=False, \n",
    "                          fit_intercept=True,\n",
    "                          intercept_scaling=1,\n",
    "                          l1_ratio=None,\n",
    "                          max_iter=100,\n",
    "                          multi_class='auto', \n",
    "                          n_jobs=-1,\n",
    "                          penalty='l2', \n",
    "                          random_state=random_state, \n",
    "                          solver='liblinear', \n",
    "                          tol=0.0001,\n",
    "                          verbose=0, \n",
    "                          warm_start=False)\n",
    "# Previous, less verbose code: LogisticRegression(random_state=random_state, n_jobs=-1) \n",
    "\n",
    "CLF = VotingClassifier(estimators=[('svm', CLF1), ('lr', CLF2)], \n",
    "                       voting='soft', \n",
    "                       n_jobs=-1, \n",
    "                       weights=None)\n",
    "# Previous, less verbose code: VotingClassifier(estimators=[('svm', CLF1), ('lr', CLF2)], voting='soft', n_jobs=-1)\n",
    "\n",
    "# Loading dataset \n",
    "corpus, y = parse_dataset(DATASET_FP)\n",
    "\n",
    "X = bisectioned_embeddings_avg(corpus)\n",
    "\n",
    "if EXTRA_FEATURES:\n",
    "    \n",
    "    extraFeatures = np.load(open('train_feats_taskA.npy','rb'))\n",
    "    indices = np.load(open('./indices', 'rb'), allow_pickle=True)\n",
    "    \n",
    "    extraFeatures =[extraF[indices] for extraF in extraFeatures]\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        X[i] = np.concatenate((X[i],extraFeatures[i]))\n",
    "\n",
    "class_counts = np.asarray(np.unique(y, return_counts=True)).T.tolist()\n",
    "print (\"class counts:\",class_counts)\n",
    "\n",
    "# Returns an array of the same size as 'y' where each entry is a prediction obtained by cross validated\n",
    "predicted = cross_val_predict(CLF, X, y, cv=K_FOLDS)\n",
    "\n",
    "# Modify F1-score calculation depending on the task\n",
    "if TASK.lower() == 'a':\n",
    "    score = metrics.f1_score(y, predicted, pos_label=1)\n",
    "    p = metrics.precision_score(y, predicted, pos_label=1)\n",
    "    r = metrics.recall_score(y, predicted, pos_label=1)\n",
    "    acc = metrics.accuracy_score(y, predicted)\n",
    "elif TASK.lower() == 'b':\n",
    "    # if you set average to None, it will return results for each class separately \n",
    "    score = metrics.f1_score(y, predicted, average=None)\n",
    "    score_ = metrics.f1_score(y, predicted, average='macro')\n",
    "    p = metrics.precision_score(y, predicted, average=\"macro\")\n",
    "    r = metrics.recall_score(y, predicted, average=\"macro\")\n",
    "    acc = metrics.accuracy_score(y, predicted)\n",
    "print (\"F1-score Task\", TASK, score)\n",
    "print (\"Precision Task\", TASK, p)\n",
    "print (\"Recall Task\", TASK, r)\n",
    "print (\"Accuracy Task\", TASK, acc)\n",
    "for p in predicted:\n",
    "    PREDICTIONSFILE.write(\"{}\\n\".format(p))\n",
    "PREDICTIONSFILE.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Fit on the whole Train ...\")\n",
    "CLF.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model \n",
    "# import pickle\n",
    "# filename = 'finalized_model.sav'\n",
    "# pickle.dump(CLF, open(filename, 'wb'))\n",
    "\n",
    "## if later you want to load the model, execute the following\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Ready to TEST\")\n",
    "\n",
    "test_corpus, y  = parse_dataset('../datasets/goldtest_TaskA/SemEval2018-T3_gold_test_taskA_emoji.txt')\n",
    "\n",
    "X_test = bisectioned_embeddings_avg(test_corpus)\n",
    "print(\"len(X_test)\", len(X_test))\n",
    "\n",
    "extraFeatures = np.load(open('./test_feats.npy', 'rb'))\n",
    "extraFeatures =[extraF[indices] for extraF in extraFeatures]\n",
    "for i in range(len(X_test)):\n",
    "        X_test[i] = np.concatenate((X_test[i],extraFeatures[i]))\n",
    "\n",
    "print(\"test X dimension\",len(X_test[0]))\n",
    "\n",
    "y_test_predicted = CLF.predict(X_test)\n",
    "\n",
    "with open('predictions-taskA.txt', 'w') as f:\n",
    "    for yp in y_test_predicted:\n",
    "        f.write(str(yp)+\"\\n\")\n",
    "\n",
    "score = metrics.f1_score(y, y_test_predicted, pos_label=1)\n",
    "p = metrics.precision_score(y, y_test_predicted, pos_label=1)\n",
    "r = metrics.recall_score(y, y_test_predicted, pos_label=1)\n",
    "acc = metrics.accuracy_score(y, y_test_predicted)\n",
    "\n",
    "print (\"F1-score Task\", TASK, score)\n",
    "print (\"Precision Task\", TASK, p)\n",
    "print (\"Recall Task\", TASK, r)\n",
    "print (\"Accuracy Task\", TASK, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(CLF, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
